# import os
# import math
# from datetime import datetime, timezone, timedelta

# from dotenv import load_dotenv
# import psycopg2
# from psycopg2.extras import DictCursor

# print("ğŸ”¹ [LOG] calc_trend_score.py import ì‹œì‘")

# # 1) .env ë¡œë“œ
# load_dotenv()
# print("ğŸ”¹ [LOG] .env ë¡œë“œ ì™„ë£Œ")

# DB_HOST = os.getenv("DB_HOST")
# DB_PORT = os.getenv("DB_PORT")
# DB_NAME = os.getenv("DB_NAME")
# DB_USER = os.getenv("DB_USER")
# DB_PASSWORD = os.getenv("DB_PASSWORD")

# print(
#     f"[ENV CHECK]\nHOST={DB_HOST}\nPORT={DB_PORT}\nDB={DB_NAME}\nUSER={DB_USER}\n"
# )

# # 2) DB ì—°ê²°
# try:
#     conn = psycopg2.connect(
#         host=DB_HOST,
#         port=DB_PORT,
#         dbname=DB_NAME,
#         user=DB_USER,
#         password=DB_PASSWORD,
#     )
#     print("[LOG] DB ì—°ê²° ì„±ê³µ")
# except Exception as e:
#     print("[ERROR] DB ì—°ê²° ì‹¤íŒ¨:", e)
#     raise


# # íŠ¸ë Œë“œ ê³„ì‚° íŒŒë¼ë¯¸í„°
# RECENT_KEYWORD_DAYS = 3   # ìµœê·¼ Nì¼ ì•ˆ ê¸°ì‚¬ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
# HALF_LIFE_DAYS = 7        # 7ì¼ ì§€ë‚˜ë©´ recency_scoreê°€ 0.5ê°€ ë˜ë„ë¡
# LAMBDA = math.log(2) / HALF_LIFE_DAYS  # ì§€ìˆ˜ê°ì‡  ê³„ìˆ˜


# def fetch_analyzed_articles():
#     """
#     ANALYZED ìƒíƒœì˜ ê¸°ì‚¬ + analysis_result + published_at + result_id ê°€ì ¸ì˜¤ê¸°
#     """
#     print("[LOG] fetch_analyzed_articles() í˜¸ì¶œ")
#     with conn.cursor(cursor_factory=DictCursor) as cur:
#         cur.execute(
#             """
#             SELECT
#                 a.article_id,
#                 a.published_at,
#                 ar.result_id
#             FROM article a
#             JOIN analysis_result ar
#               ON ar.article_id = a.article_id
#             WHERE a.ingest_status = 'ANALYZED'
#               AND a.published_at IS NOT NULL;
#             """
#         )
#         rows = cur.fetchall()
#         print(f"[LOG] ANALYZED ê¸°ì‚¬ ê°œìˆ˜: {len(rows)}")
#         return rows


# def fetch_keywords_for_results():
#     """
#     ëª¨ë“  result_idì— ëŒ€í•´ ì—°ê²°ëœ í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ
#     """
#     print("[LOG] fetch_keywords_for_results() í˜¸ì¶œ")
#     with conn.cursor(cursor_factory=DictCursor) as cur:
#         cur.execute(
#             """
#             SELECT result_id, keyword
#             FROM analysis_keywords;
#             """
#         )
#         rows = cur.fetchall()

#     result_keywords = {}
#     for row in rows:
#         rid = row["result_id"]
#         kw = (row["keyword"] or "").strip()
#         if not kw:
#             continue
#         result_keywords.setdefault(rid, set()).add(kw)

#     print(f"ğŸ”¹ [LOG] í‚¤ì›Œë“œê°€ ìˆëŠ” result_id ìˆ˜: {len(result_keywords)}")
#     return result_keywords


# def compute_recency_score(published_at, now):
#     """
#     ë°œí–‰ì¼ ê¸°ì¤€ recency_score ê³„ì‚° (0~1)
#     """
#     # published_atì´ timezone ì—†ëŠ” naiveì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì²˜ë¦¬
#     if published_at.tzinfo is None:
#         published_at = published_at.replace(tzinfo=timezone.utc)

#     diff_days = (now - published_at).total_seconds() / 86400.0
#     if diff_days < 0:
#         diff_days = 0  # ë¯¸ë˜ ê¸°ì‚¬ ë°©ì–´

#     score = math.exp(-LAMBDA * diff_days)  # 0 ~ 1
#     return max(0.0, min(1.0, score))


# def main():
#     print("[LOG] calc_trend_score main() ì‹œì‘")

#     # 1) ê¸°ì¤€ ì‹œê°„ (í˜„ì¬)
#     now = datetime.now(timezone.utc)
#     print(f"ğŸ”¹ [LOG] now = {now.isoformat()}")

#     # 2) ë¶„ì„ ì™„ë£Œ ê¸°ì‚¬ + í‚¤ì›Œë“œ ì¡°íšŒ
#     articles = fetch_analyzed_articles()
#     if not articles:
#         print("[LOG] ANALYZED ìƒíƒœì˜ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
#         return

#     result_keywords = fetch_keywords_for_results()

#     # 3) ìµœê·¼ Nì¼ ì•ˆì˜ ê¸°ì‚¬ë“¤ë§Œ ë½‘ì•„ì„œ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
#     recent_cutoff = now - timedelta(days=RECENT_KEYWORD_DAYS)
#     print(
#         f"[LOG] ìµœê·¼ í‚¤ì›Œë“œ ê³„ì‚° ê¸°ì¤€: {RECENT_KEYWORD_DAYS}ì¼ (cutoff={recent_cutoff.isoformat()})"
#     )

#     # article_id -> (result_id, published_at)
#     article_map = {}
#     for row in articles:
#         article_map[row["article_id"]] = {
#             "result_id": row["result_id"],
#             "published_at": row["published_at"],
#         }

#     # keyword -> ìµœê·¼ Nì¼ ë‚´ ë“±ì¥ article ìˆ˜
#     keyword_freq = {}

#     for row in articles:
#         published_at = row["published_at"]
#         if published_at.tzinfo is None:
#             published_at = published_at.replace(tzinfo=timezone.utc)

#         if published_at < recent_cutoff:
#             continue

#         rid = row["result_id"]
#         kws = result_keywords.get(rid, set())
#         for kw in kws:
#             keyword_freq[kw] = keyword_freq.get(kw, 0) + 1

#     if not keyword_freq:
#         print("[LOG] ìµœê·¼ Nì¼ ë‚´ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. topical_scoreëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬.")
#         max_freq = 1
#     else:
#         max_freq = max(keyword_freq.values())

#     print(f"ğŸ”¹ [LOG] ìµœê·¼ Nì¼ ë‚´ ì„œë¡œ ì¶œí˜„í•œ í‚¤ì›Œë“œ ì¢…ë¥˜ ìˆ˜: {len(keyword_freq)}, ìµœëŒ€ ë¹ˆë„: {max_freq}")

#     # 4) ê° ê¸°ì‚¬ë³„ trend_score ê³„ì‚° ë° UPDATE
#     updated_count = 0

#     with conn.cursor() as cur:
#         for row in articles:
#             article_id = row["article_id"]
#             result_id = row["result_id"]
#             published_at = row["published_at"]

#             recency = compute_recency_score(published_at, now)

#             kws = result_keywords.get(result_id, set())
#             if kws and keyword_freq:
#                 # ê° í‚¤ì›Œë“œì— ëŒ€í•´ (ë¹ˆë„ / max_freq) ê³„ì‚° â†’ í‰ê· 
#                 scores = []
#                 for kw in kws:
#                     freq = keyword_freq.get(kw, 0)
#                     if freq <= 0:
#                         continue
#                     scores.append(freq / max_freq)

#                 if scores:
#                     topical = sum(scores) / len(scores)
#                 else:
#                     topical = 0.0
#             else:
#                 topical = 0.0

#             # ìµœì¢… trend_score (0~1)
#             trend_score = 0.7 * recency + 0.3 * topical
#             trend_score = round(trend_score, 2)  # ì†Œìˆ˜ 6ìë¦¬ ì •ë„ë¡œ ì œí•œ

#             print(
#                 f"ğŸ”¹ [LOG] article_id={article_id}, result_id={result_id}, "
#                 f"recency={recency:.3f}, topical={topical:.3f}, trend_score={trend_score:.3f}"
#             )

#             cur.execute(
#                 """
#                 UPDATE analysis_result
#                 SET trend_score = %s
#                 WHERE result_id = %s;
#                 """,
#                 (trend_score, result_id),
#             )
#             updated_count += 1

#     conn.commit()
#     print(f"[LOG] trend_score ì—…ë°ì´íŠ¸ ì™„ë£Œ, ëŒ€ìƒ ê¸°ì‚¬ ìˆ˜={updated_count}")
#     conn.close()
#     print("[LOG] calc_trend_score ì¢…ë£Œ, DB ì—°ê²° ë‹«ìŒ")


# if __name__ == "__main__":
#     print("[LOG] __main__ ë¸”ë¡ ì§„ì… (calc_trend_score)")
#     main()
